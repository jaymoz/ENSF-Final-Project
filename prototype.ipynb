{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n"
     ]
    }
   ],
   "source": [
    "print(\"begin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ignite'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mignite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Events, create_supervised_trainer, create_supervised_evaluator\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mignite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mignite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhandlers\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ignite'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "import ignite.metrics\n",
    "import ignite.contrib.handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data_cn\"\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice:\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEVICE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dset = datasets.CIFAR10(\n",
    "    root=DATA_DIR, train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_dset = datasets.CIFAR10(\n",
    "    root=DATA_DIR, train=False, download=True, transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormChannels(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, -1)\n",
    "        x = self.norm(x)\n",
    "        x = x.transpose(-1, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.residual = nn.Sequential(*layers)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.gamma * self.residual(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtBlock(Residual):\n",
    "    def __init__(self, channels, kernel_size, mult=4, p_drop=0.0):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        hidden_channels = channels * mult\n",
    "        super().__init__(\n",
    "            nn.Conv2d(\n",
    "                channels, channels, kernel_size, padding=padding, groups=channels\n",
    "            ),\n",
    "            LayerNormChannels(channels),\n",
    "            nn.Conv2d(channels, hidden_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(hidden_channels, channels, 1),\n",
    "            nn.Dropout(p_drop),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, stride=2):\n",
    "        super().__init__(\n",
    "            LayerNormChannels(in_channels),\n",
    "            nn.Conv2d(in_channels, out_channels, stride, stride=stride),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, num_blocks, kernel_size, p_drop=0.0):\n",
    "        layers = (\n",
    "            []\n",
    "            if in_channels == out_channels\n",
    "            else [DownsampleBlock(in_channels, out_channels)]\n",
    "        )\n",
    "        layers += [\n",
    "            ConvNeXtBlock(out_channels, kernel_size, p_drop=p_drop)\n",
    "            for _ in range(num_blocks)\n",
    "        ]\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtBody(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, in_channels, channel_list, num_blocks_list, kernel_size, p_drop=0.0\n",
    "    ):\n",
    "        layers = []\n",
    "        for out_channels, num_blocks in zip(channel_list, num_blocks_list):\n",
    "            layers.append(\n",
    "                Stage(in_channels, out_channels, num_blocks, kernel_size, p_drop)\n",
    "            )\n",
    "            in_channels = out_channels\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stem(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, patch_size):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, patch_size, stride=patch_size),\n",
    "            LayerNormChannels(out_channels),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Sequential):\n",
    "    def __init__(self, in_channels, classes):\n",
    "        super().__init__(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.LayerNorm(in_channels),\n",
    "            nn.Linear(in_channels, classes),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXt(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        classes,\n",
    "        channel_list,\n",
    "        num_blocks_list,\n",
    "        kernel_size,\n",
    "        patch_size,\n",
    "        in_channels=3,\n",
    "        res_p_drop=0.0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            Stem(in_channels, channel_list[0], patch_size),\n",
    "            ConvNeXtBody(\n",
    "                channel_list[0], channel_list, num_blocks_list, kernel_size, res_p_drop\n",
    "            ),\n",
    "            Head(channel_list[-1], classes),\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, Residual):\n",
    "                nn.init.zeros_(m.gamma)\n",
    "\n",
    "    def separate_parameters(self):\n",
    "        parameters_decay = set()\n",
    "        parameters_no_decay = set()\n",
    "        modules_weight_decay = (nn.Linear, nn.Conv2d)\n",
    "        modules_no_weight_decay = (nn.LayerNorm,)\n",
    "\n",
    "        for m_name, m in self.named_modules():\n",
    "            for param_name, param in m.named_parameters():\n",
    "                full_param_name = f\"{m_name}.{param_name}\" if m_name else param_name\n",
    "\n",
    "                if isinstance(m, modules_no_weight_decay):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif param_name.endswith(\"bias\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, Residual) and param_name.endswith(\"gamma\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, modules_weight_decay):\n",
    "                    parameters_decay.add(full_param_name)\n",
    "\n",
    "        # sanity check\n",
    "        assert len(parameters_decay & parameters_no_decay) == 0\n",
    "        assert len(parameters_decay) + len(parameters_no_decay) == len(\n",
    "            list(model.parameters())\n",
    "        )\n",
    "\n",
    "        return parameters_decay, parameters_no_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNeXt(\n",
    "    NUM_CLASSES,\n",
    "    channel_list=[64, 128, 256, 512],\n",
    "    num_blocks_list=[2, 2, 2, 2],\n",
    "    kernel_size=7,\n",
    "    patch_size=1,\n",
    "    res_p_drop=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (0): Stem(\n",
       "    (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): LayerNormChannels(\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (1): ConvNeXtBody(\n",
       "    (0): Stage(\n",
       "      (0): ConvNeXtBlock(\n",
       "        (residual): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
       "          (1): LayerNormChannels(\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ConvNeXtBlock(\n",
       "        (residual): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
       "          (1): LayerNormChannels(\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Stage(\n",
       "      (0): DownsampleBlock(\n",
       "        (0): LayerNormChannels(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (1): ConvNeXtBlock(\n",
       "        (residual): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "          (1): LayerNormChannels(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ConvNeXtBlock(\n",
       "        (residual): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "          (1): LayerNormChannels(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Stage(\n",
       "      (0): DownsampleBlock(\n",
       "        (0): LayerNormChannels(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (1): ConvNeXtBlock(\n",
       "        (residual): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "          (1): LayerNormChannels(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ConvNeXtBlock(\n",
       "        (residual): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "          (1): LayerNormChannels(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Stage(\n",
       "      (0): DownsampleBlock(\n",
       "        (0): LayerNormChannels(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (1): ConvNeXtBlock(\n",
       "        (residual): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): LayerNormChannels(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ConvNeXtBlock(\n",
       "        (residual): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): LayerNormChannels(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): Head(\n",
       "    (0): AdaptiveAvgPool2d(output_size=1)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 6,376,466\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters: {:,}\".format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, learning_rate, weight_decay):\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "    parameters_decay, parameters_no_decay = model.separate_parameters()\n",
    "\n",
    "    optim_groups = [\n",
    "        {\n",
    "            \"params\": [param_dict[pn] for pn in parameters_decay],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\"params\": [param_dict[pn] for pn in parameters_no_decay], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = optim.AdamW(optim_groups, lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = get_optimizer(model, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "trainer = create_supervised_trainer(model, optimizer, loss, device=DEVICE)\n",
    "lr_scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(train_loader), epochs=EPOCHS\n",
    ")\n",
    "trainer.add_event_handler(\n",
    "    Events.ITERATION_COMPLETED, lambda engine: lr_scheduler.step()\n",
    ")\n",
    "ignite.metrics.RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics = {\"accuracy\": ignite.metrics.Accuracy(), \"loss\": ignite.metrics.Loss(loss)}\n",
    "evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=DEVICE)\n",
    "history = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    train_state = engine.state\n",
    "    epoch = train_state.epoch\n",
    "    max_epochs = train_state.max_epochs\n",
    "    train_loss = train_state.metrics[\"loss\"]\n",
    "    history[\"train loss\"].append(train_loss)\n",
    "\n",
    "    evaluator.run(test_loader)\n",
    "    val_metrics = evaluator.state.metrics\n",
    "    val_loss = val_metrics[\"loss\"]\n",
    "    val_acc = val_metrics[\"accuracy\"]\n",
    "    history[\"val loss\"].append(val_loss)\n",
    "    history[\"val acc\"].append(val_acc)\n",
    "\n",
    "    print(\n",
    "        \"{}/{} - train: loss {:.3f}; val: loss {:.3f} accuracy {:.3f}\".format(\n",
    "            epoch, max_epochs, train_loss, val_loss, val_acc\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100 - train: loss 1.882; val: loss 1.817 accuracy 0.329\n",
      "2/100 - train: loss 1.668; val: loss 1.692 accuracy 0.392\n",
      "3/100 - train: loss 1.544; val: loss 1.546 accuracy 0.443\n",
      "4/100 - train: loss 1.491; val: loss 1.463 accuracy 0.469\n",
      "5/100 - train: loss 1.417; val: loss 1.394 accuracy 0.492\n",
      "6/100 - train: loss 1.380; val: loss 1.325 accuracy 0.526\n",
      "7/100 - train: loss 1.318; val: loss 1.289 accuracy 0.538\n",
      "8/100 - train: loss 1.229; val: loss 1.186 accuracy 0.572\n",
      "9/100 - train: loss 1.165; val: loss 1.097 accuracy 0.602\n",
      "10/100 - train: loss 1.060; val: loss 1.028 accuracy 0.640\n",
      "11/100 - train: loss 0.975; val: loss 0.966 accuracy 0.658\n",
      "12/100 - train: loss 0.875; val: loss 0.862 accuracy 0.697\n",
      "13/100 - train: loss 0.822; val: loss 0.797 accuracy 0.727\n",
      "14/100 - train: loss 0.764; val: loss 0.694 accuracy 0.760\n",
      "15/100 - train: loss 0.675; val: loss 0.634 accuracy 0.777\n",
      "16/100 - train: loss 0.662; val: loss 0.621 accuracy 0.784\n",
      "17/100 - train: loss 0.619; val: loss 0.635 accuracy 0.780\n",
      "18/100 - train: loss 0.568; val: loss 0.614 accuracy 0.791\n",
      "19/100 - train: loss 0.552; val: loss 0.572 accuracy 0.803\n",
      "20/100 - train: loss 0.525; val: loss 0.543 accuracy 0.809\n",
      "21/100 - train: loss 0.517; val: loss 0.616 accuracy 0.787\n",
      "22/100 - train: loss 0.498; val: loss 0.515 accuracy 0.824\n",
      "23/100 - train: loss 0.501; val: loss 0.519 accuracy 0.823\n",
      "24/100 - train: loss 0.451; val: loss 0.510 accuracy 0.825\n",
      "25/100 - train: loss 0.460; val: loss 0.516 accuracy 0.821\n",
      "26/100 - train: loss 0.482; val: loss 0.525 accuracy 0.823\n",
      "27/100 - train: loss 0.454; val: loss 0.586 accuracy 0.800\n",
      "28/100 - train: loss 0.448; val: loss 0.589 accuracy 0.796\n",
      "29/100 - train: loss 0.396; val: loss 0.449 accuracy 0.846\n",
      "30/100 - train: loss 0.397; val: loss 0.436 accuracy 0.851\n",
      "31/100 - train: loss 0.372; val: loss 0.474 accuracy 0.842\n",
      "32/100 - train: loss 0.398; val: loss 0.477 accuracy 0.845\n",
      "33/100 - train: loss 0.401; val: loss 0.491 accuracy 0.834\n",
      "34/100 - train: loss 0.394; val: loss 0.483 accuracy 0.837\n",
      "35/100 - train: loss 0.367; val: loss 0.435 accuracy 0.853\n",
      "36/100 - train: loss 0.365; val: loss 0.418 accuracy 0.858\n",
      "37/100 - train: loss 0.385; val: loss 0.459 accuracy 0.844\n",
      "38/100 - train: loss 0.345; val: loss 0.415 accuracy 0.857\n",
      "39/100 - train: loss 0.349; val: loss 0.414 accuracy 0.860\n",
      "40/100 - train: loss 0.323; val: loss 0.472 accuracy 0.847\n",
      "41/100 - train: loss 0.316; val: loss 0.444 accuracy 0.855\n",
      "42/100 - train: loss 0.331; val: loss 0.405 accuracy 0.865\n",
      "43/100 - train: loss 0.286; val: loss 0.409 accuracy 0.867\n",
      "44/100 - train: loss 0.301; val: loss 0.441 accuracy 0.853\n",
      "45/100 - train: loss 0.301; val: loss 0.471 accuracy 0.843\n",
      "46/100 - train: loss 0.297; val: loss 0.432 accuracy 0.854\n",
      "47/100 - train: loss 0.281; val: loss 0.482 accuracy 0.843\n",
      "48/100 - train: loss 0.286; val: loss 0.383 accuracy 0.874\n",
      "49/100 - train: loss 0.257; val: loss 0.392 accuracy 0.872\n",
      "50/100 - train: loss 0.249; val: loss 0.385 accuracy 0.873\n",
      "51/100 - train: loss 0.251; val: loss 0.409 accuracy 0.866\n",
      "52/100 - train: loss 0.269; val: loss 0.392 accuracy 0.867\n",
      "53/100 - train: loss 0.271; val: loss 0.387 accuracy 0.871\n",
      "54/100 - train: loss 0.243; val: loss 0.385 accuracy 0.870\n",
      "55/100 - train: loss 0.232; val: loss 0.389 accuracy 0.870\n",
      "56/100 - train: loss 0.197; val: loss 0.354 accuracy 0.883\n",
      "57/100 - train: loss 0.222; val: loss 0.413 accuracy 0.867\n",
      "58/100 - train: loss 0.200; val: loss 0.379 accuracy 0.879\n",
      "59/100 - train: loss 0.196; val: loss 0.372 accuracy 0.885\n",
      "60/100 - train: loss 0.206; val: loss 0.358 accuracy 0.889\n",
      "61/100 - train: loss 0.202; val: loss 0.390 accuracy 0.873\n",
      "62/100 - train: loss 0.162; val: loss 0.383 accuracy 0.884\n",
      "63/100 - train: loss 0.154; val: loss 0.363 accuracy 0.884\n",
      "64/100 - train: loss 0.164; val: loss 0.369 accuracy 0.884\n",
      "65/100 - train: loss 0.138; val: loss 0.366 accuracy 0.887\n",
      "66/100 - train: loss 0.147; val: loss 0.404 accuracy 0.879\n",
      "67/100 - train: loss 0.116; val: loss 0.381 accuracy 0.886\n",
      "68/100 - train: loss 0.105; val: loss 0.383 accuracy 0.889\n",
      "69/100 - train: loss 0.109; val: loss 0.398 accuracy 0.886\n",
      "70/100 - train: loss 0.077; val: loss 0.378 accuracy 0.898\n",
      "71/100 - train: loss 0.088; val: loss 0.375 accuracy 0.894\n",
      "72/100 - train: loss 0.096; val: loss 0.364 accuracy 0.898\n",
      "73/100 - train: loss 0.075; val: loss 0.366 accuracy 0.900\n",
      "74/100 - train: loss 0.067; val: loss 0.381 accuracy 0.894\n",
      "75/100 - train: loss 0.064; val: loss 0.401 accuracy 0.894\n",
      "76/100 - train: loss 0.066; val: loss 0.384 accuracy 0.897\n",
      "77/100 - train: loss 0.053; val: loss 0.388 accuracy 0.899\n",
      "78/100 - train: loss 0.043; val: loss 0.406 accuracy 0.902\n",
      "79/100 - train: loss 0.036; val: loss 0.390 accuracy 0.901\n",
      "80/100 - train: loss 0.046; val: loss 0.378 accuracy 0.905\n",
      "81/100 - train: loss 0.027; val: loss 0.407 accuracy 0.903\n",
      "82/100 - train: loss 0.025; val: loss 0.403 accuracy 0.903\n",
      "83/100 - train: loss 0.019; val: loss 0.425 accuracy 0.904\n",
      "84/100 - train: loss 0.020; val: loss 0.387 accuracy 0.912\n",
      "85/100 - train: loss 0.010; val: loss 0.408 accuracy 0.907\n",
      "86/100 - train: loss 0.011; val: loss 0.391 accuracy 0.911\n",
      "87/100 - train: loss 0.015; val: loss 0.414 accuracy 0.909\n",
      "88/100 - train: loss 0.006; val: loss 0.412 accuracy 0.909\n",
      "89/100 - train: loss 0.007; val: loss 0.402 accuracy 0.913\n",
      "90/100 - train: loss 0.003; val: loss 0.422 accuracy 0.912\n",
      "91/100 - train: loss 0.003; val: loss 0.420 accuracy 0.916\n",
      "92/100 - train: loss 0.006; val: loss 0.410 accuracy 0.912\n",
      "93/100 - train: loss 0.002; val: loss 0.411 accuracy 0.915\n",
      "94/100 - train: loss 0.001; val: loss 0.415 accuracy 0.913\n",
      "95/100 - train: loss 0.001; val: loss 0.424 accuracy 0.914\n",
      "96/100 - train: loss 0.000; val: loss 0.419 accuracy 0.914\n",
      "97/100 - train: loss 0.001; val: loss 0.418 accuracy 0.913\n",
      "98/100 - train: loss 0.002; val: loss 0.416 accuracy 0.915\n",
      "99/100 - train: loss 0.001; val: loss 0.415 accuracy 0.915\n",
      "100/100 - train: loss 0.001; val: loss 0.415 accuracy 0.915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 156300\n",
       "\tepoch: 100\n",
       "\tepoch_length: 1563\n",
       "\tmax_epochs: 100\n",
       "\toutput: 0.004967710934579372\n",
       "\tbatch: <class 'list'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(train_loader, max_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./model_cn\"\n",
    "\n",
    "torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnex1 = ConvNeXt(\n",
    "    NUM_CLASSES,\n",
    "    channel_list=[64, 128, 256, 512],\n",
    "    num_blocks_list=[2, 2, 2, 2],\n",
    "    kernel_size=7,\n",
    "    patch_size=1,\n",
    "    res_p_drop=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnex1.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXt(\n",
      "  (0): Stem(\n",
      "    (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): LayerNormChannels(\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (1): ConvNeXtBody(\n",
      "    (0): Stage(\n",
      "      (0): ConvNeXtBlock(\n",
      "        (residual): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
      "          (1): LayerNormChannels(\n",
      "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): GELU(approximate='none')\n",
      "          (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): ConvNeXtBlock(\n",
      "        (residual): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
      "          (1): LayerNormChannels(\n",
      "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): GELU(approximate='none')\n",
      "          (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Stage(\n",
      "      (0): DownsampleBlock(\n",
      "        (0): LayerNormChannels(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (1): ConvNeXtBlock(\n",
      "        (residual): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (1): LayerNormChannels(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): GELU(approximate='none')\n",
      "          (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): ConvNeXtBlock(\n",
      "        (residual): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (1): LayerNormChannels(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): GELU(approximate='none')\n",
      "          (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Stage(\n",
      "      (0): DownsampleBlock(\n",
      "        (0): LayerNormChannels(\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (1): ConvNeXtBlock(\n",
      "        (residual): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (1): LayerNormChannels(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): GELU(approximate='none')\n",
      "          (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): ConvNeXtBlock(\n",
      "        (residual): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (1): LayerNormChannels(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): GELU(approximate='none')\n",
      "          (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Stage(\n",
      "      (0): DownsampleBlock(\n",
      "        (0): LayerNormChannels(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (1): ConvNeXtBlock(\n",
      "        (residual): Sequential(\n",
      "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (1): LayerNormChannels(\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): GELU(approximate='none')\n",
      "          (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): ConvNeXtBlock(\n",
      "        (residual): Sequential(\n",
      "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (1): LayerNormChannels(\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): GELU(approximate='none')\n",
      "          (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): Head(\n",
      "    (0): AdaptiveAvgPool2d(output_size=1)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(cnex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
